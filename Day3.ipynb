{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82e8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required in order to run the import below.\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40f1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\clair\\anaconda3\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\clair\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\clair\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\clair\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\clair\\anaconda3\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\clair\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Instructing folks not to think too much about this code, just know that we're doing it \n",
    "# because it is going and getting the package from the internet and downloading it to our computer\n",
    "# so that we can use the code.\n",
    "\n",
    "# This will only be included if you don't install nltk on day 3.\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2106e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll go over some of the file I/O and string functions, just to make sure everyone\n",
    "# remembers.\n",
    "\n",
    "# Storing text to save to a file.\n",
    "essential_text = \"We definitely want to remember the way in which communities need agency over their own histories!\"\n",
    "\n",
    "# Opening a file, and write our essential text to the file, and close the file. \n",
    "with open('essential.txt', 'w') as file:\n",
    "    file.write(essential_text)\n",
    "    \n",
    "# Opening a file, read the text from it, and store the text in a variable.\n",
    "with open('essential.txt', 'r') as file:\n",
    "    essential_text_from_file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde52615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We definitely want to remember the way in which communities need agency over their own histories!\n"
     ]
    }
   ],
   "source": [
    "# Let's look at what we just stored.\n",
    "print(essential_text_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929c1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\clair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\clair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\clair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\clair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can we annotate some parts of this? Yes! Using NLTK!\n",
    "\n",
    "# Let's download some essential NLTK packages for today.\n",
    "import nltk.downloader\n",
    "nltk.download('averaged_perceptron_tagger') # For POS tagger.\n",
    "nltk.download('punkt') # For word tokenizer.\n",
    "nltk.download('tagsets') # For UPenn tagset help.\n",
    "nltk.download('wordnet') # For WordNet lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158d909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NLTK tokenize function (for words).\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Broadly, tokenization is the process of preparing input for a particular model.\n",
    "# In an NLP subfield known as \"lexical analysis\", tokenization is the process\n",
    "# of demarcating sections of a string character, typically using spaces to separate\n",
    "# words and/or sentences. Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73c6796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'definitely', 'want', 'to', 'remember', 'the', 'way', 'in', 'which', 'communities', 'need', 'agency', 'over', 'their', 'own', 'histories', '!']\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenize the sentence we just read in!\n",
    "tokenized_text = word_tokenize(essential_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# What information from the sentence is lost during this process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c437929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a tokenizer, we could also use a\n",
    "# \"lemmatizer\".\n",
    "\n",
    "# A lemmatizer uses lemmatization to find all\n",
    "# \"lemmas\" of a term. I know, confusing, but\n",
    "# here are some examples:\n",
    "\n",
    "# rocks --> rock\n",
    "# corpora --> corpus\n",
    "# better --> good\n",
    "\n",
    "# In linguistic terms, a lemma is\n",
    "# a representative of a set of terms\n",
    "# by which they can be indexed.\n",
    "# For instance \"break\" is the lemma\n",
    "# for \"break\", \"breaks\", \"broke\",\n",
    "# \"broken\", and \"breaking\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81c3007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "communities : community\n"
     ]
    }
   ],
   "source": [
    "# Let's import the lemmatizer!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# And test!\n",
    "print(\"communities\" + ' : ' + lemmatizer.lemmatize(\"communities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fbe2e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We : We\n",
      "definitely : definitely\n",
      "want : want\n",
      "to : to\n",
      "remember : remember\n",
      "the : the\n",
      "way : way\n",
      "in : in\n",
      "which : which\n",
      "communities : community\n",
      "need : need\n",
      "agency : agency\n",
      "over : over\n",
      "their : their\n",
      "own : own\n",
      "histories : history\n",
      "! : !\n"
     ]
    }
   ],
   "source": [
    "# Nice!\n",
    "# Let's try to print the lemmas for the full sentence:\n",
    "for token in tokenized_text:\n",
    "    print(token + \" : \" + lemmatizer.lemmatize(token))\n",
    "    \n",
    "# Try your own sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b8680c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('definitely', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('remember', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('communities', 'NNS'),\n",
       " ('need', 'VBP'),\n",
       " ('agency', 'NN'),\n",
       " ('over', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('own', 'JJ'),\n",
       " ('histories', 'NNS'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yes! The relationship the words have to one another is more difficult to determine.\n",
    "# For instance \"their\" and \"own\" as modifiers of \"histories\", which gives \"histories\" a\n",
    "# slightly different contextual meaning.\n",
    "\n",
    "# Note other issues with this like polish/Polish, clip/clip, foil/foil.\n",
    "\n",
    "# Would determining part of speech (POS) help? Let's try it!\n",
    "\n",
    "# Use the NLTK POS (Part Of Speech) tagger on the tokenized\n",
    "# sentence (note that the sentence must be tokenized first).\n",
    "from nltk import pos_tag\n",
    "pos_tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f274d64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# Okay... but what do these abbreviations all mean?\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac2d28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spaCy==2.3.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (1.20.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (2.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (4.59.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (7.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (0.7.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spaCy==2.3.2) (1.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy==2.3.2) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy==2.3.2) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy==2.3.2) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy==2.3.2) (2.10)\n"
     ]
    }
   ],
   "source": [
    "# It would be nice if we could visualize the connections between the words though--\n",
    "# we know which are adjectives, but what do they modify?\n",
    "\n",
    "# Never fear, the spaCy package is here!\n",
    "\n",
    "# We can also do this using the spaCy package!\n",
    "\n",
    "# If we want to make sure we install a specific version\n",
    "# we can use this:\n",
    "!{sys.executable} -m pip install spaCy==2.3.2\n",
    "\n",
    "# We can also try this:\n",
    "#!{sys.executable} -m pip install spaCy\n",
    "\n",
    "# Or this:\n",
    "# `!{sys.executable} -m pip install spaCy --use-feature=2020-resolver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca60c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.20.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.59.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\clair\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Now, we do have to download a neural network English core model.\n",
    "# This is essentially a \"model\" of how English works, with rules based on millions\n",
    "# of \"correct\" sentences plugged into a neural network.\n",
    "\n",
    "# This would normally be tons of work, so many thanks to the spaCy developers for\n",
    "# providing it!\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2aeb931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this block doesn't run, run\n",
    "# `{sys.executable} -m spacy download en`\n",
    "# and then re-run!\n",
    "import en_core_web_sm # Importing the neural network model we just downloaded.\n",
    "import spacy as sp # Loading spaCy.\n",
    "\n",
    "# Load the English core model.\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdff8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our previous sentence as a spaCy object!\n",
    "spacy_sen = nlp(essential_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f797aa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We definitely want to remember the way in which communities need agency over their own histories!\n"
     ]
    }
   ],
   "source": [
    "# Print the sentence text, now that it's a spaCy object!\n",
    "print(spacy_sen.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c3fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remember : VERB\n"
     ]
    }
   ],
   "source": [
    "# Oh... it looks the same.\n",
    "\n",
    "# Well let's see if POS information is retained.\n",
    "print(spacy_sen[4].text + \" : \" + spacy_sen[4].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75791d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We : PRON\n",
      "definitely : ADV\n",
      "want : VERB\n",
      "to : PART\n",
      "remember : VERB\n",
      "the : DET\n",
      "way : NOUN\n",
      "in : ADP\n",
      "which : DET\n",
      "communities : NOUN\n",
      "need : VERB\n",
      "agency : NOUN\n",
      "over : ADP\n",
      "their : DET\n",
      "own : ADJ\n",
      "histories : NOUN\n",
      "! : PUNCT\n"
     ]
    }
   ],
   "source": [
    "# We could even print every word like this:\n",
    "for word in spacy_sen:\n",
    "    print(word.text + \" : \" + word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6500ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remember : verb, base form\n"
     ]
    }
   ],
   "source": [
    "# We can even go more in-depth using the 'explain' parameter:\n",
    "print(spacy_sen[4].text + \" : \" + sp.explain(spacy_sen[4].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfb7ffcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"459728c8ec034d2d90dcb7629eae8a6d-0\" class=\"displacy\" width=\"1410\" height=\"307.0\" direction=\"ltr\" style=\"max-width: none; height: 307.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">We</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"135\">definitely</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"135\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"220\">want</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"220\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"305\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"305\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"390\">remember</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"390\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"475\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"475\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"560\">way</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"560\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"645\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"645\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"730\">which</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"730\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"815\">communities</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"815\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"900\">need</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"900\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"985\">agency</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"985\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1070\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1070\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1155\">their</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1155\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1240\">own</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1240\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"217.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1325\">histories!</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1325\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-0\" stroke-width=\"2px\" d=\"M70,172.0 C70,87.0 210.0,87.0 210.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,174.0 L62,162.0 78,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-1\" stroke-width=\"2px\" d=\"M155,172.0 C155,129.5 205.0,129.5 205.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M155,174.0 L147,162.0 163,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-2\" stroke-width=\"2px\" d=\"M325,172.0 C325,129.5 375.0,129.5 375.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M325,174.0 L317,162.0 333,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-3\" stroke-width=\"2px\" d=\"M240,172.0 C240,87.0 380.0,87.0 380.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M380.0,174.0 L388.0,162.0 372.0,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-4\" stroke-width=\"2px\" d=\"M495,172.0 C495,129.5 545.0,129.5 545.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M495,174.0 L487,162.0 503,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-5\" stroke-width=\"2px\" d=\"M410,172.0 C410,87.0 550.0,87.0 550.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550.0,174.0 L558.0,162.0 542.0,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-6\" stroke-width=\"2px\" d=\"M665,172.0 C665,44.5 895.0,44.5 895.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M665,174.0 L657,162.0 673,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-7\" stroke-width=\"2px\" d=\"M665,172.0 C665,129.5 715.0,129.5 715.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M715.0,174.0 L723.0,162.0 707.0,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-8\" stroke-width=\"2px\" d=\"M835,172.0 C835,129.5 885.0,129.5 885.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M835,174.0 L827,162.0 843,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-9\" stroke-width=\"2px\" d=\"M580,172.0 C580,2.0 900.0,2.0 900.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M900.0,174.0 L908.0,162.0 892.0,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-10\" stroke-width=\"2px\" d=\"M920,172.0 C920,129.5 970.0,129.5 970.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970.0,174.0 L978.0,162.0 962.0,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-11\" stroke-width=\"2px\" d=\"M920,172.0 C920,87.0 1060.0,87.0 1060.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1060.0,174.0 L1068.0,162.0 1052.0,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-12\" stroke-width=\"2px\" d=\"M1175,172.0 C1175,87.0 1315.0,87.0 1315.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1175,174.0 L1167,162.0 1183,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-13\" stroke-width=\"2px\" d=\"M1260,172.0 C1260,129.5 1310.0,129.5 1310.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1260,174.0 L1252,162.0 1268,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-459728c8ec034d2d90dcb7629eae8a6d-0-14\" stroke-width=\"2px\" d=\"M1090,172.0 C1090,44.5 1320.0,44.5 1320.0,172.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-459728c8ec034d2d90dcb7629eae8a6d-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1320.0,174.0 L1328.0,162.0 1312.0,162.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# But words also have explicit relationships to one another, as we discussed earlier.\n",
    "# How do we see those? Well let's try to visualize them!\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(spacy_sen, style='dep', jupyter=True, options={'distance': 85})\n",
    "\n",
    "# If you aren't using Jupyter, you'll need to use:\n",
    "#\n",
    "# `displacy.serve(spacy_sen, style='dep', options={'distance': 120})\n",
    "#\n",
    "# Then you'll see the following:\n",
    "#\n",
    "# `Serving on port 5000...\n",
    "#  Using the 'dep' visualizer`\n",
    "#\n",
    "# To view, go to the following address in your web browser:\n",
    "#\n",
    "# `http://127.0.0.1:5000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88a4ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very pretty! That helps us understand the \"structure\" of the sentence.\n",
    "\n",
    "# Let's try visualizing your own sentence!\n",
    "\n",
    "# (If we have time, we'll look into named entity recognition)\n",
    "\n",
    "# Let's take our coffee break :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e952cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# Next, we're going to look into \"finding\" a string a bit.\n",
    "\n",
    "# Let's start with the sentence we had above.\n",
    "\n",
    "# First we'll define a search term.\n",
    "string_to_search_1 = \"communities\"\n",
    "\n",
    "# Next we'll loop through all of the \"tokens\" in the tokenized text and look for a match!\n",
    "for token in tokenized_text:\n",
    "    if token == string_to_search_1: # Why do we have 2 equal signs here?\n",
    "        print(\"Found it! :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0814a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n",
      "Didn't find it :(\n"
     ]
    }
   ],
   "source": [
    "# Okay! But what if we want to know if a word, doesn't exist, like if we search \"bigfoot\"\n",
    "# what happens?\n",
    "string_to_search_2 = \"bigfoot\"\n",
    "\n",
    "for token in tokenized_text:\n",
    "    if token == string_to_search_2:\n",
    "        print(\"Found it! :)\")\n",
    "    else:\n",
    "        print(\"Didn't find it :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f6980b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find it :(\n"
     ]
    }
   ],
   "source": [
    "# Oh no it printed a bunch! That's a bit overwhelming though...\n",
    "# Can we have it print just once?\n",
    "string_to_search_2 = \"bigfoot\"\n",
    "\n",
    "# Let's define a \"found it\" variable.\n",
    "# We'll set it to \"false\" outside of the loop.\n",
    "found_it = False\n",
    "\n",
    "for token in tokenized_text:\n",
    "    if token == string_to_search_2:\n",
    "        found_it = True # Set to true when found.\n",
    "        \n",
    "if found_it:\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Didn't find it :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e42a3141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# That's it! Much more succinct.\n",
    "\n",
    "# But do we even need the loop? Is there a faster way to do this?\n",
    "\n",
    "# Yes! Using \"in\", we can do this:\n",
    "if string_to_search_1 in tokenized_text:\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Didn't find it :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "babdc668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# We actually don't even need the tokenized text for this!\n",
    "if string_to_search_1 in essential_text:\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Didn't find it :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3486db1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found! :(\n"
     ]
    }
   ],
   "source": [
    "# This seems a little too good to be true.\n",
    "# How do you think this could go wrong?\n",
    "\n",
    "# What happens if a user has caps lock on?\n",
    "string_to_search_3 = \"COMMUNITIES\"\n",
    "\n",
    "if string_to_search_3 in essential_text:\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Not found! :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29eeeb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# Oh no. That's not good.\n",
    "# Well let's send the text to uppercase maybe?\n",
    "uppercase_essential_text = essential_text.upper()\n",
    "\n",
    "if string_to_search_3 in uppercase_essential_text:\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Not found! :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18d7ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found! :(\n"
     ]
    }
   ],
   "source": [
    "# It worked! But what about the original search with \"communities\"?\n",
    "uppercase_essential_text = essential_text.upper()\n",
    "\n",
    "if string_to_search_1 in uppercase_essential_text:\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Not found! :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c8062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# ...that's disappointed.\n",
    "# Wait! What if we send the search text AND the essential text to lowercase?\n",
    "lowercase_essential_text = essential_text.lower()\n",
    "\n",
    "if string_to_search_3.lower() in lowercase_essential_text:\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Not found! :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15a23e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's it!\n",
    "\n",
    "# But what information is lost when we do this?\n",
    "\n",
    "# A consistent theme here is that, we can manipulate data in so many ways,\n",
    "# but all manipulations will mean some information is lost.\n",
    "# This is important to keep in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d7f38ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we want to know the position of the \"find\"? This is especially important if\n",
    "# we have strings that are thousands of pages long.\n",
    "\n",
    "# For that, we can use the \"find\" function in Python.\n",
    "# Documentation: https://www.geeksforgeeks.org/python-string-find/\n",
    "\n",
    "# Let's try it!\n",
    "lowercase_essential_text.find(string_to_search_3.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f40dc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now we know where the find is!\n",
    "\n",
    "# What happens if the string isn't there?\n",
    "lowercase_essential_text.find(string_to_search_2.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7db81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We definitely want to remember the way in which [HERE'S THE FIND!]communities need agency over their own histories!\n"
     ]
    }
   ],
   "source": [
    "# We get a \"-1\"!\n",
    "\n",
    "# But having the number isn't always useful... what if we could indicate where the \"find\" is?\n",
    "\n",
    "# Let's insert [HERE'S THE FIND!] in the string.\n",
    "\n",
    "position_to_insert = lowercase_essential_text.find(string_to_search_3.lower())\n",
    "\n",
    "# For this we need to do string concatenation:\n",
    "new_essential_text = essential_text[:position_to_insert] + \"[HERE'S THE FIND!]\" + essential_text[position_to_insert:]\n",
    "\n",
    "print(new_essential_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef1d3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice!\n",
    "\n",
    "# But what if we have multiple matches? Or we want multiple matches?\n",
    "\n",
    "# If time, try to find a string in another sentence.\n",
    "\n",
    "# For that, we need the regular expressions package, which we'll look into after the break:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c4d2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are regular expressions?\n",
    "# A regular expression (often shortened as regex) is a sequence of characters that specifies\n",
    "# a particular search pattern, usually for string-searching algorithms.\n",
    "\n",
    "# To use regular expressions in Python, we need to import the \"re\" package:\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b06db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it :)\n"
     ]
    }
   ],
   "source": [
    "# Let's try to do our uppercase search problem using the re package!\n",
    "if re.search(string_to_search_3, essential_text, re.IGNORECASE):\n",
    "    print(\"Found it :)\")\n",
    "else:\n",
    "    print(\"Not found :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9d4decd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# Yay! It worked!\n",
    "\n",
    "# But what if have a partial match in a sentence?\n",
    "string_to_search_4 = 'cat'\n",
    "less_essential_text = \"I'm not looking for something sophisticated.\"\n",
    "\n",
    "if re.search(string_to_search_4, less_essential_text, re.IGNORECASE):\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Not found! :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3349827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uh-oh.\n",
    "# That's a problem.\n",
    "# Okay, well, can we make our search system better?\n",
    "\n",
    "# Regular expressions have all sorts of weird rules, and often\n",
    "# feel like another programming language in and of themselves.\n",
    "# Luckily, a number of sites provide \"cheatsheets\" and testing\n",
    "# materials, so you can test before you run.\n",
    "\n",
    "# I personally recommend: https://regexr.com/, but there\n",
    "# are tons out there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b310ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For instance, let's try to make a regular expression that\n",
    "# mimics \"re.IGNORECASE\" to start.\n",
    "\n",
    "# Let's try to find \"communities\" or \"COMMUNITIES\" in the\n",
    "# 'essential_text':\n",
    "# \"We definitely want to remember the way in which communities need agency over their own histories!\"\n",
    "#\n",
    "# One way to do this is:\n",
    "# (COMMUNITIES)|(communities)\n",
    "\n",
    "# But that won't match \"Communities\" hmm...\n",
    "#\n",
    "# We could do this:\n",
    "# [Cc][Oo][Mm][Mm][Uu][Nn][Ii][Tt][Ii][Ee][Ss]\n",
    "\n",
    "# But that's a bit long...\n",
    "#\n",
    "# Let's do this instead. Click on the \"flags\" option\n",
    "# at regexr.com. Select the 'case insensitive' option.\n",
    "#\n",
    "# Then we'll have this:\n",
    "# (communities)/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a70df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n",
      "Found it! :)\n",
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# Well... we have our regex working on the web, but how do we put it in Python?\n",
    "if re.search(r'(COMMUNITIES)|(communities)', essential_text):\n",
    "    print(\"Found it! :)\")\n",
    "    \n",
    "if re.search(r'[Cc][Oo][Mm][Mm][Uu][Nn][Ii][Tt][Ii][Ee][Ss]', essential_text):\n",
    "    print(\"Found it! :)\")\n",
    "    \n",
    "if re.search(r'(communities)', essential_text, flags=re.I):\n",
    "    print(\"Found it! :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfff2d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found! :O\n"
     ]
    }
   ],
   "source": [
    "# Let's return to our 'cat' example.\n",
    "#\n",
    "# There's a special flag, \"\\b\" (backspace) which moves the\n",
    "# cursor backwards on position (on the same 'row').\n",
    "#\n",
    "# With regular expressions, this functions as a 'word boundary'.\n",
    "# This means that it attempts to differentiate between \"word\" and\n",
    "# \"non-word\" characters.\n",
    "\n",
    "# For instance:\n",
    "if re.search(r\"\\b\" + re.escape(string_to_search_4) + r\"\\b\", less_essential_text, re.IGNORECASE):\n",
    "    print(\"Found it! :P\")\n",
    "else:\n",
    "    print(\"Not found! :O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6531dfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found! :(\n"
     ]
    }
   ],
   "source": [
    "# Nice!\n",
    "\n",
    "# Let's finish today attempting to search for a \n",
    "# \"concept\" (so to speak).\n",
    "#\n",
    "# For instance, notice what happens when we try to search\n",
    "# 'community' in the essential text, even with our best regular\n",
    "# expressions:\n",
    "if re.search(r\"\\b\" + re.escape('community') + r\"\\b\", essential_text, re.IGNORECASE):\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Not found! :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd143b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We definitely want to remember the way in which community need agency over their own history !\n"
     ]
    }
   ],
   "source": [
    "# But if someone searches that, they probably want to match; how can we help?\n",
    "\n",
    "# Hmm... Did we talk about anything earlier that could be used for that?\n",
    "\n",
    "# Of course, the lemmatizer!\n",
    "\n",
    "# Let's lemmatize the sentence:\n",
    "lemmatized_essential_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "\n",
    "# And return the lemmatizer form to a sentence:\n",
    "lemmatized_essential_text_str = \" \".join(lemmatized_essential_text)\n",
    "print(lemmatized_essential_text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96160f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it! :)\n"
     ]
    }
   ],
   "source": [
    "# We also need to lemmatize the search string:\n",
    "lemmatized_search_string = lemmatizer.lemmatize(string_to_search_3.lower())\n",
    "# Remember to send to lowercase or the lemmatizer won't work!\n",
    "\n",
    "# And now we'll search!\n",
    "if re.search(r\"\\b\" + re.escape(lemmatized_search_string) + r\"\\b\", lemmatized_essential_text_str, re.IGNORECASE):\n",
    "    print(\"Found it! :)\")\n",
    "else:\n",
    "    print(\"Not found! :(\")\n",
    "    \n",
    "# Let's try with your own sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba633f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stage of HIV infection are acute infection ( also known a primary infection ) , latency and AIDS . Acute infection last for several week and may include symptom such a fever , swollen lymph node , inflammation of the throat , rash , muscle pain , malaise , and mouth and esophageal sore . The latency stage involves few or no symptom and can last anywhere from two week to twenty year or more , depending on the individual . AIDS , the final stage of HIV infection , is defined by low CD4+ T cell count ( fewer than 200 per L ) , various opportunistic infection , cancer and other condition .\n"
     ]
    }
   ],
   "source": [
    "# But wait! What if we want the positions where multiple string matches are?\n",
    "\n",
    "# Let's try this new text block:\n",
    "string_to_search_5 = \"HIV\"\n",
    "lemmatized_search_string_2 = lemmatizer.lemmatize(string_to_search_5.lower())\n",
    "\n",
    "# This is the first paragraph of this Wikipedia page:\n",
    "# https://en.wikipedia.org/wiki/Signs_and_symptoms_of_HIV/AIDS\n",
    "new_essential_text = \"The stages of HIV infection are acute infection (also known as primary infection), latency and AIDS. Acute infection lasts for several weeks and may include symptoms such as fever, swollen lymph nodes, inflammation of the throat, rash, muscle pain, malaise, and mouth and esophageal sores. The latency stage involves few or no symptoms and can last anywhere from two weeks to twenty years or more, depending on the individual. AIDS, the final stage of HIV infection, is defined by low CD4+ T cell counts (fewer than 200 per L), various opportunistic infections, cancers and other conditions.\"\n",
    "new_tokenized_text = word_tokenize(new_essential_text)\n",
    "new_lemmatized_essential_text = [lemmatizer.lemmatize(word) for word in new_tokenized_text]\n",
    "new_lemmatized_essential_text_str = \" \".join(new_lemmatized_essential_text)\n",
    "print(new_lemmatized_essential_text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d209ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIV\n",
      "HIV\n"
     ]
    }
   ],
   "source": [
    "# Nice!\n",
    "# Next, we'll use the \"re.findall()\" function:\n",
    "matches = re.findall(r\"\\b\" + re.escape(lemmatized_search_string_2) + r\"\\b\", new_lemmatized_essential_text_str, re.IGNORECASE)\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bab59c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 455]\n"
     ]
    }
   ],
   "source": [
    "# Okay but we want to know *where* these matches are.\n",
    "# For this we use the \"re.finditer()\" function.\n",
    "matches = re.finditer(r\"\\b\" + re.escape(lemmatized_search_string_2) + r\"\\b\", new_lemmatized_essential_text_str, re.IGNORECASE)\n",
    "\n",
    "results = [i.start() for i in matches]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3cacdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stage of [HERE'S THE FIND!]HIV infection are acute infection ( also known a primary infection ) , latency and AIDS . Acute infection last for several week and may include symptom such a fever , swollen lymph node , inflammation of the throat , rash , muscle pain , malaise , and mouth and esophageal sore . The latency stage involves few or no symptom and can last anywhere from two week to twenty year or more , depending on the individual . AIDS , the final stage of [HERE'S THE FIND!]HIV infection , is defined by low CD4+ T cell count ( fewer than 200 per L ) , various opportunistic infection , cancer and other condition .\n"
     ]
    }
   ],
   "source": [
    "# Let's insert [HERE'S THE FIND!] in the string, just like last time.\n",
    "position_to_insert = lowercase_essential_text.find(string_to_search_3.lower())\n",
    "\n",
    "# We need to know how long the string we are entering is, in order\n",
    "# to know where to but subsequent matches, which are moved over.\n",
    "thing_to_insert = \"[HERE'S THE FIND!]\"\n",
    "length_of_insert = len(thing_to_insert)\n",
    "\n",
    "very_new_lemmatized_essential_text_str = new_lemmatized_essential_text_str\n",
    "\n",
    "no_inserted = 0\n",
    "\n",
    "for match_span in results:\n",
    "    very_new_lemmatized_essential_text_str = very_new_lemmatized_essential_text_str[:(int(match_span)+(no_inserted * length_of_insert))] + thing_to_insert + very_new_lemmatized_essential_text_str[(int(match_span)+(no_inserted * length_of_insert)):] \n",
    "    no_inserted += 1\n",
    "    \n",
    "print(very_new_lemmatized_essential_text_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
